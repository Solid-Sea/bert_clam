{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT-CLAM æ¡†æ¶å¿«é€Ÿå…¥é—¨\n",
    "\n",
    "æœ¬æ•™ç¨‹å°†æŒ‡å¯¼æ‚¨å¦‚ä½•ä½¿ç”¨ BERT-CLAM æ¡†æ¶è¿›è¡ŒæŒç»­å­¦ä¹ ä»»åŠ¡ã€‚\n",
    "\n",
    "## ç›®å½•\n",
    "1. ç¯å¢ƒè®¾ç½®\n",
    "2. åŠ è½½æ•°æ®\n",
    "3. åˆå§‹åŒ–æ¨¡å‹å’Œæ¨¡å—\n",
    "4. åˆ›å»ºå’Œç»„åˆç­–ç•¥\n",
    "5. æ³¨å…¥ç­–ç•¥å¹¶è®­ç»ƒ\n",
    "6. æ€»ç»“"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç¯å¢ƒè®¾ç½®\n",
    "\n",
    "é¦–å…ˆï¼Œç¡®ä¿å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„ä¾èµ–ï¼š\n",
    "\n",
    "```bash\n",
    "pip install torch transformers datasets faiss-cpu\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# å¯¼å…¥BERT-CLAMç»„ä»¶\n",
    "from bert_clam.models.bert_clam_model import BERTCLAMModel\n",
    "from bert_clam.core.ewc import EnhancedElasticWeightConsolidation\n",
    "from bert_clam.core.memory_replay_bank import EnhancedAdaptiveMemoryRetrieval\n",
    "from bert_clam.core.grammar_aware import EnhancedGrammarAwareModule\n",
    "from bert_clam.core.strategy import EWCStrategy, MRBStrategy, GrammarStrategy\n",
    "\n",
    "print(\"âœ“ æ‰€æœ‰ä¾èµ–å·²æˆåŠŸå¯¼å…¥\")\n",
    "print(f\"âœ“ PyTorchç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"âœ“ è®¾å¤‡: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. åŠ è½½æ•°æ®\n",
    "\n",
    "ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬åˆ›å»ºä¸€äº›è™šæ‹Ÿæ•°æ®ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œæ‚¨åº”è¯¥ä½¿ç”¨çœŸå®çš„æ•°æ®é›†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆå§‹åŒ–tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# åˆ›å»ºè™šæ‹Ÿæ•°æ®\n",
    "def create_dummy_data(num_samples=32, max_length=128):\n",
    "    \"\"\"åˆ›å»ºè™šæ‹Ÿè®­ç»ƒæ•°æ®\"\"\"\n",
    "    texts = [\n",
    "        \"This is a positive example.\",\n",
    "        \"This is a negative example.\",\n",
    "        \"Another positive sentence here.\",\n",
    "        \"Yet another negative sentence.\"\n",
    "    ] * (num_samples // 4)\n",
    "    \n",
    "    labels = [1, 0, 1, 0] * (num_samples // 4)\n",
    "    \n",
    "    # Tokenize\n",
    "    encodings = tokenizer(\n",
    "        texts,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # åˆ›å»ºæ•°æ®é›†\n",
    "    dataset = []\n",
    "    for i in range(len(texts)):\n",
    "        dataset.append({\n",
    "            'input_ids': encodings['input_ids'][i],\n",
    "            'attention_mask': encodings['attention_mask'][i],\n",
    "            'labels': torch.tensor(labels[i])\n",
    "        })\n",
    "    \n",
    "    return DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# åˆ›å»ºä¸¤ä¸ªä»»åŠ¡çš„æ•°æ®\n",
    "task0_dataloader = create_dummy_data(num_samples=32)\n",
    "task1_dataloader = create_dummy_data(num_samples=32)\n",
    "\n",
    "print(\"âœ“ æ•°æ®åŠ è½½å®Œæˆ\")\n",
    "print(f\"  - ä»»åŠ¡0: {len(task0_dataloader)} æ‰¹æ¬¡\")\n",
    "print(f\"  - ä»»åŠ¡1: {len(task1_dataloader)} æ‰¹æ¬¡\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. åˆå§‹åŒ–æ¨¡å‹å’Œæ¨¡å—\n",
    "\n",
    "åˆ›å»ºBERT-CLAMæ¨¡å‹å®ä¾‹ä»¥åŠå„ä¸ªæŒç»­å­¦ä¹ æ¨¡å—ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¾ç½®è®¾å¤‡\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# åˆå§‹åŒ–æ ¸å¿ƒæ¨¡å—\n",
    "ewc_module = EnhancedElasticWeightConsolidation(\n",
    "    lambda_ewc=0.5,\n",
    "    fisher_samples=10\n",
    ")\n",
    "\n",
    "mrb_module = EnhancedAdaptiveMemoryRetrieval(\n",
    "    hidden_size=768,\n",
    "    k=5,\n",
    "    memory_dim=768\n",
    ")\n",
    "\n",
    "grammar_module = EnhancedGrammarAwareModule(\n",
    "    hidden_size=768,\n",
    "    num_attention_heads=12,\n",
    "    grammar_features_dim=64\n",
    ")\n",
    "\n",
    "print(\"âœ“ æ ¸å¿ƒæ¨¡å—åˆå§‹åŒ–å®Œæˆ\")\n",
    "print(f\"  - EWCæ¨¡å—: lambda={ewc_module.ewc_core.lambda_ewc}\")\n",
    "print(f\"  - MRBæ¨¡å—: k={mrb_module.k}\")\n",
    "print(f\"  - Grammaræ¨¡å—: dim={grammar_module.grammar_core.grammar_features_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. åˆ›å»ºå’Œç»„åˆç­–ç•¥\n",
    "\n",
    "ä½¿ç”¨ç­–ç•¥æ¨¡å¼å°†å„ä¸ªæ¨¡å—ç»„åˆæˆä¸€ä¸ªç»Ÿä¸€çš„æŒç»­å­¦ä¹ æµç¨‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºåŸºç¡€æ¨¡å‹ï¼ˆä¸å¯ç”¨ä»»ä½•æ¨¡å—ï¼Œæˆ‘ä»¬å°†é€šè¿‡ç­–ç•¥æ³¨å…¥ï¼‰\n",
    "model = BERTCLAMModel(\n",
    "    model_name='bert-base-uncased',\n",
    "    num_labels=2,\n",
    "    lora_r=8,\n",
    "    lora_alpha=16,\n",
    "    device=device,\n",
    "    lora_enabled=True,\n",
    "    enable_ewc=False,  # é€šè¿‡ç­–ç•¥å¯ç”¨\n",
    "    enable_amr=False,  # é€šè¿‡ç­–ç•¥å¯ç”¨\n",
    "    enable_grammar=False  # é€šè¿‡ç­–ç•¥å¯ç”¨\n",
    ").to(device)\n",
    "\n",
    "# åˆ›å»ºç­–ç•¥åˆ—è¡¨\n",
    "strategies = [\n",
    "    GrammarStrategy(grammar_module.to(device)),\n",
    "    MRBStrategy(mrb_module.to(device), fusion_weight=0.2),\n",
    "    EWCStrategy(ewc_module, model)\n",
    "]\n",
    "\n",
    "# å°†ç­–ç•¥æ³¨å…¥æ¨¡å‹\n",
    "model.strategies = strategies\n",
    "\n",
    "print(\"âœ“ ç­–ç•¥åˆ›å»ºå¹¶æ³¨å…¥å®Œæˆ\")\n",
    "print(f\"  - ç­–ç•¥æ•°é‡: {len(model.strategies)}\")\n",
    "for i, strategy in enumerate(model.strategies):\n",
    "    print(f\"  - ç­–ç•¥{i+1}: {strategy.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. æ³¨å…¥ç­–ç•¥å¹¶è®­ç»ƒ\n",
    "\n",
    "ç°åœ¨æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ³¨å…¥äº†ç­–ç•¥çš„æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚è¿™é‡Œæˆ‘ä»¬è¿›è¡Œä¸€ä¸ªæå°è§„æ¨¡çš„è®­ç»ƒæ¼”ç¤ºã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¾ç½®ä¼˜åŒ–å™¨\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# è®­ç»ƒå‡½æ•°\n",
    "def train_task(model, dataloader, task_id, num_steps=5):\n",
    "    \"\"\"è®­ç»ƒå•ä¸ªä»»åŠ¡\"\"\"\n",
    "    model.train()\n",
    "    model.register_task(task_id)\n",
    "    \n",
    "    total_loss = 0\n",
    "    step = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        if step >= num_steps:\n",
    "            break\n",
    "        \n",
    "        # å°†æ•°æ®ç§»åˆ°è®¾å¤‡\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # å‰å‘ä¼ æ’­\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            task_id=task_id\n",
    "        )\n",
    "        \n",
    "        loss = outputs['loss']\n",
    "        \n",
    "        # åå‘ä¼ æ’­\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        step += 1\n",
    "        \n",
    "        print(f\"  æ­¥éª¤ {step}/{num_steps}, æŸå¤±: {loss.item():.4f}\")\n",
    "    \n",
    "    avg_loss = total_loss / step\n",
    "    print(f\"âœ“ ä»»åŠ¡{task_id}è®­ç»ƒå®Œæˆï¼Œå¹³å‡æŸå¤±: {avg_loss:.4f}\")\n",
    "    \n",
    "    # ä¿å­˜ä»»åŠ¡æ£€æŸ¥ç‚¹ï¼ˆç”¨äºEWCï¼‰\n",
    "    model.save_task_checkpoint(task_id, dataloader)\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "# è®­ç»ƒä»»åŠ¡0\n",
    "print(\"\\n=== è®­ç»ƒä»»åŠ¡0 ===\")\n",
    "loss_task0 = train_task(model, task0_dataloader, task_id=0, num_steps=3)\n",
    "\n",
    "# è®­ç»ƒä»»åŠ¡1\n",
    "print(\"\\n=== è®­ç»ƒä»»åŠ¡1 ===\")\n",
    "loss_task1 = train_task(model, task1_dataloader, task_id=1, num_steps=3)\n",
    "\n",
    "print(\"\\nâœ“ æ‰€æœ‰ä»»åŠ¡è®­ç»ƒå®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. è¯„ä¼°å’Œæ€»ç»“\n",
    "\n",
    "è®©æˆ‘ä»¬éªŒè¯æ¨¡å‹å¯ä»¥æ­£å¸¸æ¨ç†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¯„ä¼°å‡½æ•°\n",
    "def evaluate(model, dataloader, task_id):\n",
    "    \"\"\"è¯„ä¼°æ¨¡å‹\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                task_id=task_id\n",
    "            )\n",
    "            \n",
    "            predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "# è¯„ä¼°ä¸¤ä¸ªä»»åŠ¡\n",
    "print(\"\\n=== è¯„ä¼°ç»“æœ ===\")\n",
    "acc_task0 = evaluate(model, task0_dataloader, task_id=0)\n",
    "acc_task1 = evaluate(model, task1_dataloader, task_id=1)\n",
    "\n",
    "print(f\"ä»»åŠ¡0å‡†ç¡®ç‡: {acc_task0:.2%}\")\n",
    "print(f\"ä»»åŠ¡1å‡†ç¡®ç‡: {acc_task1:.2%}\")\n",
    "print(f\"å¹³å‡å‡†ç¡®ç‡: {(acc_task0 + acc_task1) / 2:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ€»ç»“\n",
    "\n",
    "åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæ‚¨å­¦ä¹ äº†ï¼š\n",
    "\n",
    "1. âœ“ å¦‚ä½•åˆå§‹åŒ–BERT-CLAMæ¡†æ¶çš„æ ¸å¿ƒç»„ä»¶\n",
    "2. âœ“ å¦‚ä½•åˆ›å»ºå’Œç»„åˆæŒç»­å­¦ä¹ ç­–ç•¥\n",
    "3. âœ“ å¦‚ä½•å°†ç­–ç•¥æ³¨å…¥æ¨¡å‹å¹¶è¿›è¡Œè®­ç»ƒ\n",
    "4. âœ“ å¦‚ä½•åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¿›è¡ŒæŒç»­å­¦ä¹ \n",
    "\n",
    "### ä¸‹ä¸€æ­¥\n",
    "\n",
    "- æŸ¥çœ‹ `README.md` äº†è§£æ›´é«˜çº§çš„ç”¨æ³•\n",
    "- æ¢ç´¢ `configs/` ç›®å½•ä¸­çš„é…ç½®æ–‡ä»¶ç¤ºä¾‹\n",
    "- å°è¯•åœ¨çœŸå®æ•°æ®é›†ä¸Šä½¿ç”¨æ¡†æ¶\n",
    "- è‡ªå®šä¹‰æ‚¨è‡ªå·±çš„æŒç»­å­¦ä¹ ç­–ç•¥\n",
    "\n",
    "### ç›¸å…³èµ„æº\n",
    "\n",
    "- [é¡¹ç›®æ–‡æ¡£](../README.md)\n",
    "- [é…ç½®æ–‡ä»¶ç¤ºä¾‹](../configs/)\n",
    "- [å•å…ƒæµ‹è¯•](../tests/)\n",
    "\n",
    "ç¥æ‚¨ä½¿ç”¨æ„‰å¿«ï¼ğŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}