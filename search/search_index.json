{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"BERT-CLAM: A Modular Framework for Continual Learning Research","text":"<p>BERT-CLAM is a flexible, production-ready framework for prototyping and comparing continual learning strategies on NLP tasks. Built on BERT and designed with modularity in mind, it enables researchers and developers to quickly experiment with different combinations of continual learning techniques.</p>"},{"location":"#overview","title":"Overview","text":"<p>BERT-CLAM transforms continual learning research from hardcoded experiments into a composable, strategy-driven framework. Instead of modifying model code for each experiment, you simply configure which strategies to apply and in what order.</p> <p>Key Value Proposition: - Rapid Prototyping: Test new continual learning ideas in hours, not days - Fair Comparisons: Standardized evaluation across different strategies - Production-Ready: Clean architecture suitable for real-world deployment</p>"},{"location":"#core-features","title":"Core Features","text":""},{"location":"#modular-by-design","title":"Modular by Design","text":"<p>BERT-CLAM uses the Strategy Pattern to decouple continual learning techniques from the core model.</p>"},{"location":"#built-in-strategies","title":"Built-in Strategies","text":"<ul> <li>EWC: Elastic Weight Consolidation - Prevents catastrophic forgetting via regularization</li> <li>MRB: Memory Replay Bank - Retrieves and fuses knowledge from past tasks</li> <li>ALP: Adaptive LoRA Pooling - Dynamically combines task-specific adaptations</li> <li>Grammar: Grammar-Aware Attention - Leverages syntactic structure for better generalization</li> </ul>"},{"location":"#reproducible-experiments","title":"Reproducible Experiments","text":"<p>All experiments are driven by JSON configuration files. Run experiments with a single command:</p> <p>```bash python run_experiment.py --config configs/example_strategy_config.json</p>"},{"location":"api/bert_clam_model/","title":"Main Model API","text":"<p>This section documents the main BERT-CLAM model class.</p>"},{"location":"api/bert_clam_model/#bertclammodel","title":"BERTCLAMModel","text":"<p>The main BERT-CLAM model that combines BERT with continual learning strategies.</p>"},{"location":"api/bert_clam_model/#methods","title":"Methods","text":""},{"location":"api/bert_clam_model/#__init__model_namebert-base-uncased-num_labels2-lora_r8-lora_alpha16-amr_k10-ewc_lambda015-alp_top_k3-grammar_features_dim64-devicenone-lora_enabledtrue-enable_ewcfalse-enable_amrfalse-enable_alpfalse-enable_grammarfalse-strategiesnone","title":"<code>__init__(model_name='bert-base-uncased', num_labels=2, lora_r=8, lora_alpha=16, amr_k=10, ewc_lambda=0.15, alp_top_k=3, grammar_features_dim=64, device=None, lora_enabled=True, enable_ewc=False, enable_amr=False, enable_alp=False, enable_grammar=False, strategies=None)</code>","text":"<p>Initializes the BERT-CLAM model with the specified configuration.</p> <p>Parameters: - <code>model_name</code> (str): Name of the pre-trained BERT model to use (default: 'bert-base-uncased') - <code>num_labels</code> (int): Number of output labels for classification (default: 2) - <code>lora_r</code> (int): Rank parameter for LoRA adaptation (default: 8) - <code>lora_alpha</code> (int): Scaling parameter for LoRA (default: 16) - <code>amr_k</code> (int): Number of items to retrieve from memory replay bank (default: 10) - <code>ewc_lambda</code> (float): Regularization strength for EWC (default: 0.15) - <code>alp_top_k</code> (int): Number of top LoRA adapters to use in Adaptive LoRA Pooling (default: 3) - <code>grammar_features_dim</code> (int): Dimension of grammar features (default: 64) - <code>device</code> (str): Device to run the model on (default: automatically detected) - <code>lora_enabled</code> (bool): Whether to enable LoRA adaptation (default: True) - <code>enable_ewc</code> (bool): Whether to enable EWC (default: False, will be controlled by strategy) - <code>enable_amr</code> (bool): Whether to enable AMR (default: False, will be controlled by strategy) - <code>enable_alp</code> (bool): Whether to enable ALP (default: False, will be controlled by strategy) - <code>enable_grammar</code> (bool): Whether to enable grammar awareness (default: False, will be controlled by strategy) - <code>strategies</code> (List[ContinualLearningStrategy]): List of strategies to apply (default: [])</p>"},{"location":"api/bert_clam_model/#forwardinput_ids-attention_masknone-token_type_idsnone-labelsnone-task_id0","title":"<code>forward(input_ids, attention_mask=None, token_type_ids=None, labels=None, task_id=0)</code>","text":"<p>Performs a forward pass through the model.</p> <p>Parameters: - <code>input_ids</code> (torch.Tensor): Input token IDs - <code>attention_mask</code> (torch.Tensor, optional): Attention mask - <code>token_type_ids</code> (torch.Tensor, optional): Token type IDs - <code>labels</code> (torch.Tensor, optional): Ground truth labels for training - <code>task_id</code> (int): ID of the current task</p> <p>Returns: - Dict[str, Any]: Dictionary containing logits, sequence output, pooled output, hidden states, attentions, and optionally loss</p>"},{"location":"api/bert_clam_model/#get_task_embeddinginput_ids-attention_masknone","title":"<code>get_task_embedding(input_ids, attention_mask=None)</code>","text":"<p>Gets a task embedding for the given inputs.</p> <p>Parameters: - <code>input_ids</code> (torch.Tensor): Input token IDs - <code>attention_mask</code> (torch.Tensor, optional): Attention mask</p> <p>Returns: - torch.Tensor: Task embedding</p>"},{"location":"api/bert_clam_model/#register_tasktask_id","title":"<code>register_task(task_id)</code>","text":"<p>Registers a new task with the model.</p> <p>Parameters: - <code>task_id</code> (int): ID of the task to register</p>"},{"location":"api/bert_clam_model/#update_memoryinput_ids-attention_mask-labels-task_id","title":"<code>update_memory(input_ids, attention_mask, labels, task_id)</code>","text":"<p>Updates the memory systems with new task data.</p> <p>Parameters: - <code>input_ids</code> (torch.Tensor): Input token IDs - <code>attention_mask</code> (torch.Tensor): Attention mask - <code>labels</code> (torch.Tensor): Ground truth labels - <code>task_id</code> (int): ID of the current task</p>"},{"location":"api/bert_clam_model/#compute_ewc_losstask_idsnone","title":"<code>compute_ewc_loss(task_ids=None)</code>","text":"<p>Computes the EWC loss for the specified tasks.</p> <p>Parameters: - <code>task_ids</code> (List[int], optional): List of task IDs to compute loss for</p> <p>Returns: - torch.Tensor: EWC loss value</p>"},{"location":"api/bert_clam_model/#save_task_checkpointtask_id-dataloadernone","title":"<code>save_task_checkpoint(task_id, dataloader=None)</code>","text":"<p>Saves a checkpoint for the specified task (used for EWC).</p> <p>Parameters: - <code>task_id</code> (int): ID of the task to save - <code>dataloader</code> (DataLoader, optional): Dataloader for the task</p>"},{"location":"api/bert_clam_model/#get_grammar_featuresinput_ids-attention_mask","title":"<code>get_grammar_features(input_ids, attention_mask)</code>","text":"<p>Gets grammar features for the input.</p> <p>Parameters: - <code>input_ids</code> (torch.Tensor): Input token IDs - <code>attention_mask</code> (torch.Tensor): Attention mask</p> <p>Returns: - torch.Tensor: Grammar features</p>"},{"location":"api/strategy/","title":"Core Strategies API","text":"<p>This section documents the core strategy classes in the BERT-CLAM framework.</p>"},{"location":"api/strategy/#continuallearningstrategy","title":"ContinualLearningStrategy","text":"<p>The abstract base class for all continual learning strategies.</p>"},{"location":"api/strategy/#methods","title":"Methods","text":""},{"location":"api/strategy/#__init__name-str","title":"<code>__init__(name: str)</code>","text":"<p>Initializes the strategy with a name.</p> <p>Parameters: - <code>name</code> (str): The name of the strategy</p>"},{"location":"api/strategy/#applyhidden_states-model_output-task_id-kwargs","title":"<code>apply(hidden_states, model_output, task_id, **kwargs)</code>","text":"<p>Abstract method to apply the strategy to hidden states.</p> <p>Parameters: - <code>hidden_states</code> (torch.Tensor): Current hidden states [batch, seq_len, hidden_size] - <code>model_output</code> (Dict[str, Any]): Model output dictionary (with attentions, pooled_output, etc.) - <code>task_id</code> (int): Current task ID - <code>**kwargs</code>: Additional parameters (task_memory, task_embeddings, etc.)</p> <p>Returns: - Tuple[torch.Tensor, Optional[torch.Tensor]]: (enhanced_hidden_states, optional_loss)</p>"},{"location":"api/strategy/#ewcstrategy","title":"EWCStrategy","text":"<p>The Elastic Weight Consolidation strategy prevents catastrophic forgetting through regularization.</p>"},{"location":"api/strategy/#methods_1","title":"Methods","text":""},{"location":"api/strategy/#__init__ewc_module-model","title":"<code>__init__(ewc_module, model)</code>","text":"<p>Initializes the EWC strategy.</p> <p>Parameters: - <code>ewc_module</code>: The EWC module implementation - <code>model</code>: The BERT-CLAM model instance</p>"},{"location":"api/strategy/#applyhidden_states-model_output-task_id-kwargs_1","title":"<code>apply(hidden_states, model_output, task_id, **kwargs)</code>","text":"<p>Applies the EWC strategy.</p> <p>The EWC strategy does not modify the hidden states but computes a regularization loss to prevent catastrophic forgetting.</p> <p>Returns: - Tuple[torch.Tensor, Optional[torch.Tensor]]: (original_hidden_states, ewc_loss)</p>"},{"location":"api/strategy/#mrbstrategy","title":"MRBStrategy","text":"<p>The Memory Replay Bank strategy retrieves and fuses knowledge from past tasks.</p>"},{"location":"api/strategy/#methods_2","title":"Methods","text":""},{"location":"api/strategy/#__init__mrb_module-fusion_weight-float-02","title":"<code>__init__(mrb_module, fusion_weight: float = 0.2)</code>","text":"<p>Initializes the MRB strategy.</p> <p>Parameters: - <code>mrb_module</code>: The MRB module implementation - <code>fusion_weight</code> (float): Weight for fusing retrieved knowledge with current states</p>"},{"location":"api/strategy/#applyhidden_states-model_output-task_id-kwargs_2","title":"<code>apply(hidden_states, model_output, task_id, **kwargs)</code>","text":"<p>Applies the MRB strategy.</p> <p>Returns: - Tuple[torch.Tensor, Optional[torch.Tensor]]: (fused_hidden_states, None)</p>"},{"location":"api/strategy/#alpstrategy","title":"ALPStrategy","text":"<p>The Adaptive LoRA Pooling strategy dynamically combines task-specific adaptations.</p>"},{"location":"api/strategy/#methods_3","title":"Methods","text":""},{"location":"api/strategy/#__init__alp_module","title":"<code>__init__(alp_module)</code>","text":"<p>Initializes the ALP strategy.</p> <p>Parameters: - <code>alp_module</code>: The ALP module implementation</p>"},{"location":"api/strategy/#applyhidden_states-model_output-task_id-kwargs_3","title":"<code>apply(hidden_states, model_output, task_id, **kwargs)</code>","text":"<p>Applies the ALP strategy.</p> <p>Returns: - Tuple[torch.Tensor, Optional[torch.Tensor]]: (enhanced_hidden_states, None)</p>"},{"location":"api/strategy/#grammarstrategy","title":"GrammarStrategy","text":"<p>The Grammar-Aware strategy leverages syntactic structure for better generalization.</p>"},{"location":"api/strategy/#methods_4","title":"Methods","text":""},{"location":"api/strategy/#__init__grammar_module","title":"<code>__init__(grammar_module)</code>","text":"<p>Initializes the Grammar strategy.</p> <p>Parameters: - <code>grammar_module</code>: The grammar module implementation</p>"},{"location":"api/strategy/#applyhidden_states-model_output-task_id-kwargs_4","title":"<code>apply(hidden_states, model_output, task_id, **kwargs)</code>","text":"<p>Applies the Grammar strategy.</p> <p>Returns: - Tuple[torch.Tensor, Optional[torch.Tensor]]: (enhanced_hidden_states, optional_grammar_loss)</p>"},{"location":"concepts/architecture/","title":"Architecture Deep Dive","text":"<p>This section provides a detailed overview of the BERT-CLAM framework architecture.</p>"},{"location":"concepts/architecture/#strategy-pattern-implementation","title":"Strategy Pattern Implementation","text":"<p>The framework uses a clean strategy pattern where each continual learning technique is encapsulated in its own strategy class:</p> <ul> <li><code>ContinualLearningStrategy</code>: Abstract base class</li> <li><code>EWCStrategy</code>, <code>MRBStrategy</code>, <code>ALPStrategy</code>, <code>GrammarStrategy</code>: Concrete implementations</li> <li><code>BERTCLAMModel</code>: Orchestrates strategy execution</li> </ul> <p>The strategy pattern allows for flexible combination of different continual learning techniques without modifying core model code.</p>"},{"location":"concepts/architecture/#execution-flow","title":"Execution Flow","text":"<p>The framework processes data through a chain of strategies:</p> <pre><code>current_output = sequence_output\nfor strategy in self.strategies:\n    current_output, strategy_loss = strategy.apply(\n        hidden_states=current_output,\n        model_output=backbone_outputs,\n        task_id=task_id\n    )\n</code></pre> <p>This design enables:</p> <ol> <li>Modularity: Each strategy is self-contained and can be enabled/disabled independently</li> <li>Composability: Strategies can be combined in any order</li> <li>Extensibility: New strategies can be added without modifying existing code</li> </ol>"},{"location":"concepts/architecture/#core-components","title":"Core Components","text":""},{"location":"concepts/architecture/#bert-backbone","title":"BERT Backbone","text":"<p>The framework is built on top of BERT and includes:</p> <ul> <li>Enhanced BERT backbone with additional features</li> <li>Support for various BERT-based models</li> <li>Proper handling of hidden states and attention weights</li> </ul>"},{"location":"concepts/architecture/#lora-integration","title":"LoRA Integration","text":"<ul> <li>Parameter-efficient fine-tuning using Low-Rank Adaptation (LoRA)</li> <li>Multi-task LoRA support for continual learning scenarios</li> <li>Dynamic adapter management for different tasks</li> </ul>"},{"location":"concepts/architecture/#memory-systems","title":"Memory Systems","text":"<p>The framework includes several memory-based components:</p> <ul> <li>Memory Replay Bank (MRB): Stores and retrieves knowledge from past tasks</li> <li>Task Memory: Maintains representations of previously learned tasks</li> <li>Adaptive Memory: Dynamically adjusts memory usage based on task requirements</li> </ul>"},{"location":"concepts/architecture/#continual-learning-modules","title":"Continual Learning Modules","text":"<ul> <li>Elastic Weight Consolidation (EWC): Prevents catastrophic forgetting through regularization</li> <li>Adaptive LoRA Pooling (ALP): Dynamically combines task-specific adaptations</li> <li>Grammar-Aware Attention: Leverages syntactic structure for better generalization</li> </ul>"},{"location":"concepts/architecture/#model-configuration","title":"Model Configuration","text":"<p>The <code>BERTCLAMModel</code> can be configured with various parameters:</p> <pre><code>model = BERTCLAMModel(\n    model_name='bert-base-uncased',      # Base model\n    num_labels=2,                        # Number of output labels\n    lora_r=8,                           # LoRA rank\n    lora_alpha=16,                      # LoRA scaling factor\n    amr_k=10,                           # Memory retrieval size\n    ewc_lambda=0.15,                    # EWC regularization strength\n    alp_top_k=3,                        # Top-k for ALP\n    grammar_features_dim=64,            # Grammar feature dimension\n    device=device,                      # Computation device\n    lora_enabled=True,                  # Enable LoRA\n    enable_ewc=False,                   # Enable EWC (will be controlled by strategy)\n    enable_amr=False,                   # Enable AMR (will be controlled by strategy)\n    enable_alp=False,                   # Enable ALP (will be controlled by strategy)\n    enable_grammar=False,               # Enable grammar (will be controlled by strategy)\n    strategies=[]                       # List of strategies to apply\n)\n</code></pre>"},{"location":"concepts/architecture/#task-management","title":"Task Management","text":"<p>The framework provides mechanisms for managing multiple tasks:</p> <ul> <li><code>register_task(task_id)</code>: Register a new task and initialize task-specific components</li> <li><code>update_memory(...)</code>: Update memory systems with new task data</li> <li><code>get_task_embedding(...)</code>: Obtain embeddings representing task characteristics</li> <li><code>save_task_checkpoint(...)</code>: Save task-specific information (e.g., for EWC)</li> </ul>"},{"location":"concepts/architecture/#loss-computation","title":"Loss Computation","text":"<p>The framework combines multiple loss components:</p> <ul> <li>Cross-entropy loss: Primary classification loss</li> <li>Strategy-specific losses: Losses from individual strategies (EWC, grammar-aware, etc.)</li> <li>Knowledge distillation loss: Loss for preserving knowledge from previous tasks</li> </ul> <p>This architecture enables researchers to easily experiment with different combinations of continual learning techniques while maintaining a clean, modular codebase.</p>"},{"location":"concepts/strategy_pattern/","title":"Strategy Pattern","text":"<p>The Strategy Pattern is the core architectural pattern that makes BERT-CLAM flexible and modular. This document explains how to add your own continual learning strategies.</p>"},{"location":"concepts/strategy_pattern/#overview","title":"Overview","text":"<p>The Strategy Pattern allows you to define a family of algorithms, encapsulate each one, and make them interchangeable. In BERT-CLAM, each continual learning technique is implemented as a strategy that can be applied to the model's hidden states.</p>"},{"location":"concepts/strategy_pattern/#adding-a-custom-strategy","title":"Adding a Custom Strategy","text":"<p>Adding a custom continual learning strategy takes 5 simple steps:</p>"},{"location":"concepts/strategy_pattern/#step-1-implement-the-strategy-class","title":"Step 1: Implement the Strategy Class","text":"<p>Create a new strategy class that inherits from <code>ContinualLearningStrategy</code>:</p> <pre><code>from bert_clam.core.strategy import ContinualLearningStrategy\n\nclass MyCustomStrategy(ContinualLearningStrategy):\n    def __init__(self, my_module):\n        super().__init__(\"MyCustom\")\n        self.module = my_module\n\n    def apply(self, hidden_states, model_output, task_id, **kwargs):\n        enhanced_states = self.module(hidden_states)\n        return enhanced_states, None\n</code></pre> <p>The <code>apply</code> method should: - Take the current hidden states, model output, and task ID as input - Apply the strategy's logic to the hidden states - Return the enhanced hidden states and an optional loss tensor - Use the <code>**kwargs</code> parameter to access additional context (like task memory, embeddings, etc.)</p>"},{"location":"concepts/strategy_pattern/#step-2-register-in-strategy-factory","title":"Step 2: Register in Strategy Factory","text":"<p>You can register your strategy by importing it in your experiment script and using it directly, or by adding it to a strategy factory if you implement one.</p>"},{"location":"concepts/strategy_pattern/#step-3-update-model-initialization","title":"Step 3: Update Model Initialization","text":"<p>When creating your model, add your custom strategy to the strategies list:</p> <pre><code>from bert_clam.models.bert_clam_model import BERTCLAMModel\n\n# Create your strategy instance\nmy_strategy = MyCustomStrategy(my_module)\n\n# Create the model without enabling built-in modules\nmodel = BERTCLAMModel(\n    model_name='bert-base-uncased',\n    num_labels=2,\n    lora_r=8,\n    lora_alpha=16,\n    device=device,\n    lora_enabled=True,\n    enable_ewc=False,      # Will be controlled by strategy\n    enable_amr=False,      # Will be controlled by strategy\n    enable_grammar=False,  # Will be controlled by strategy\n    strategies=[my_strategy]  # Add your strategy\n)\n</code></pre>"},{"location":"concepts/strategy_pattern/#step-4-configure-in-json-optional","title":"Step 4: Configure in JSON (Optional)","text":"<p>If you want to configure your strategy through JSON files, you'll need to extend the configuration loading mechanism:</p> <pre><code>{\n  \"strategies\": [\n    {\"type\": \"my_custom\", \"enabled\": true}\n  ]\n}\n</code></pre>"},{"location":"concepts/strategy_pattern/#step-5-run-experiment","title":"Step 5: Run Experiment","text":"<pre><code>python run_experiment.py --config configs/my_custom_experiment.json\n</code></pre>"},{"location":"concepts/strategy_pattern/#strategy-interface","title":"Strategy Interface","text":"<p>All strategies must implement the <code>ContinualLearningStrategy</code> abstract base class:</p> <pre><code>from abc import ABC, abstractmethod\nfrom typing import Tuple, Optional\nimport torch\n\nclass ContinualLearningStrategy(ABC):\n    def __init__(self, name: str):\n        self.name = name\n\n    @abstractmethod\n    def apply(self,\n              hidden_states: torch.Tensor,\n              model_output: Dict[str, Any],\n              task_id: int,\n              **kwargs) -&gt; Tuple[torch.Tensor, Optional[torch.Tensor]]:\n        \"\"\"\n        Apply the strategy to hidden states\n\n        Args:\n            hidden_states: Current hidden states [batch, seq_len, hidden_size]\n            model_output: Model output dictionary (with attentions, pooled_output, etc.)\n            task_id: Current task ID\n            **kwargs: Additional parameters (task_memory, task_embeddings, etc.)\n\n        Returns:\n            (enhanced_hidden_states, optional_loss)\n        \"\"\"\n        pass\n</code></pre>"},{"location":"concepts/strategy_pattern/#existing-strategies","title":"Existing Strategies","text":""},{"location":"concepts/strategy_pattern/#ewc-strategy","title":"EWC Strategy","text":"<p>The EWC (Elastic Weight Consolidation) strategy prevents catastrophic forgetting by adding a regularization term:</p> <pre><code>class EWCStrategy(ContinualLearningStrategy):\n    def __init__(self, ewc_module, model):\n        super().__init__(\"EWC\")\n        self.ewc = ewc_module\n        self.model = model\n\n    def apply(self, hidden_states, model_output, task_id, **kwargs):\n        # EWC doesn't modify hidden states, only computes regularization loss\n        ewc_loss = None\n        if self.ewc and hasattr(self.ewc, 'compute_multi_task_ewc_loss'):\n            task_memory = kwargs.get('task_memory', {})\n            active_tasks = list(task_memory.keys())\n            if active_tasks:\n                ewc_loss = self.ewc.compute_multi_task_ewc_loss(self.model, active_tasks)\n\n        return hidden_states, ewc_loss\n</code></pre>"},{"location":"concepts/strategy_pattern/#memory-replay-bank-mrb-strategy","title":"Memory Replay Bank (MRB) Strategy","text":"<p>The MRB strategy retrieves and fuses knowledge from past tasks:</p> <pre><code>class MRBStrategy(ContinualLearningStrategy):\n    def __init__(self, mrb_module, fusion_weight: float = 0.2):\n        super().__init__(\"MRB\")\n        self.mrb = mrb_module\n        self.fusion_weight = fusion_weight\n\n    def apply(self, hidden_states, model_output, task_id, **kwargs):\n        task_memory = kwargs.get('task_memory', {})\n\n        if self.mrb and task_id in task_memory:\n            retrieved_knowledge = self.mrb(hidden_states, task_id)\n            fused_output = (1 - self.fusion_weight) * hidden_states + self.fusion_weight * retrieved_knowledge\n            return fused_output, None\n\n        return hidden_states, None\n</code></pre>"},{"location":"concepts/strategy_pattern/#adaptive-lora-pooling-alp-strategy","title":"Adaptive LoRA Pooling (ALP) Strategy","text":"<p>The ALP strategy dynamically combines task-specific adaptations:</p> <pre><code>class ALPStrategy(ContinualLearningStrategy):\n    def __init__(self, alp_module):\n        super().__init__(\"ALP\")\n        self.alp = alp_module\n\n    def apply(self, hidden_states, model_output, task_id, **kwargs):\n        task_embeddings = kwargs.get('task_embeddings', {})\n        get_task_embedding = kwargs.get('get_task_embedding')\n        input_ids = kwargs.get('input_ids')\n        attention_mask = kwargs.get('attention_mask')\n\n        if self.alp and task_id in task_embeddings and get_task_embedding is not None:\n            task_embedding = get_task_embedding(input_ids, attention_mask)\n            enhanced_output = self.alp(\n                hidden_states,\n                task_embedding,\n                'classifier',\n                task_id\n            )\n            return enhanced_output, None\n\n        return hidden_states, None\n</code></pre>"},{"location":"concepts/strategy_pattern/#grammar-strategy","title":"Grammar Strategy","text":"<p>The Grammar strategy leverages syntactic structure for better generalization:</p> <pre><code>class GrammarStrategy(ContinualLearningStrategy):\n    def __init__(self, grammar_module):\n        super().__init__(\"Grammar\")\n        self.grammar = grammar_module\n\n    def apply(self, hidden_states, model_output, task_id, **kwargs):\n        if not self.grammar:\n            return hidden_states, None\n\n        # Get attention weights\n        attentions = model_output.get('attentions')\n        attention_weights = attentions[-1] if attentions else None\n\n        # Apply grammar-aware enhancement\n        if attention_weights is not None:\n            enhanced_output = self.grammar(hidden_states, attention_weights)\n        else:\n            enhanced_output = self.grammar(hidden_states)\n\n        # Compute grammar loss\n        grammar_loss = None\n        if hasattr(self.grammar, 'compute_syntax_aware_loss'):\n            grammar_loss = self.grammar.compute_syntax_aware_loss(enhanced_output)\n\n        return enhanced_output, grammar_loss\n</code></pre>"},{"location":"concepts/strategy_pattern/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Keep strategies focused: Each strategy should implement a single, well-defined continual learning technique.</p> </li> <li> <p>Use the kwargs parameter: Pass additional context through the <code>**kwargs</code> parameter rather than hardcoding dependencies.</p> </li> <li> <p>Handle optional components gracefully: Check if modules exist before using them, and return appropriate defaults when they don't.</p> </li> <li> <p>Minimize state in strategies: Strategies should be stateless when possible, with state managed by the model or external components.</p> </li> <li> <p>Consider computational efficiency: Strategies are applied during each forward pass, so optimize for speed when possible.</p> </li> </ol> <p>The Strategy Pattern makes BERT-CLAM highly flexible, allowing researchers to easily experiment with different combinations of continual learning techniques without modifying the core model code.</p>"},{"location":"developer/testing/","title":"BERT-CLAM \u200b\u6d4b\u8bd5\u200b\u548c\u200b\u793a\u4f8b\u200b\u6307\u5357","text":"<p>\u200b\u672c\u200b\u6587\u6863\u200b\u8bf4\u660e\u200b\u5982\u4f55\u200b\u8fd0\u884c\u200b\u5355\u5143\u6d4b\u8bd5\u200b\u548c\u200b\u4f7f\u7528\u200b\u793a\u4f8b\u200b\u3002</p>"},{"location":"developer/testing/#_1","title":"\u5355\u5143\u6d4b\u8bd5","text":""},{"location":"developer/testing/#_2","title":"\u8fd0\u884c\u200b\u6240\u6709\u200b\u6d4b\u8bd5","text":"<pre><code>python -m unittest discover tests/\n</code></pre>"},{"location":"developer/testing/#_3","title":"\u8fd0\u884c\u200b\u7279\u5b9a\u200b\u6d4b\u8bd5\u200b\u6587\u4ef6","text":"<pre><code># \u200b\u6d4b\u8bd5\u200b\u8bb0\u5fc6\u200b\u91cd\u653e\u5e93\u200b\npython -m unittest tests/test_memory_replay_bank.py\n\n# \u200b\u6d4b\u8bd5\u200bEWC\u200b\u6a21\u5757\u200b\npython -m unittest tests/test_ewc.py\n\n# \u200b\u6d4b\u8bd5\u200b\u7b56\u7565\u200b\u6a21\u5757\u200b\npython -m unittest tests/test_strategies.py\n</code></pre>"},{"location":"developer/testing/#_4","title":"\u6d4b\u8bd5\u200b\u8986\u76d6","text":"<p>\u200b\u5f53\u524d\u200b\u6d4b\u8bd5\u200b\u8986\u76d6\u200b\u4ee5\u4e0b\u200b\u6838\u5fc3\u200b\u7ec4\u4ef6\u200b\uff1a</p> <ul> <li>\u2705 \u200b\u8bb0\u5fc6\u200b\u91cd\u653e\u5e93\u200b (Memory Replay Bank): \u200b\u6d4b\u8bd5\u200b\u6dfb\u52a0\u200b\u3001\u200b\u68c0\u7d22\u200b\u548c\u200b\u77e5\u8bc6\u200b\u84b8\u998f\u200b\u529f\u80fd\u200b</li> <li>\u2705 \u200b\u5f39\u6027\u200b\u6743\u91cd\u200b\u5de9\u56fa\u200b (EWC): \u200b\u6d4b\u8bd5\u200bFisher\u200b\u77e9\u9635\u200b\u8ba1\u7b97\u200b\u548c\u200bEWC\u200b\u60e9\u7f5a\u200b\u9879\u200b</li> <li>\u2705 \u200b\u7b56\u7565\u200b\u6a21\u5f0f\u200b: \u200b\u6d4b\u8bd5\u200bEWC\u3001MRB\u200b\u548c\u200bGrammar\u200b\u7b56\u7565\u200b\u7684\u200b\u5e94\u7528\u200b</li> </ul>"},{"location":"developer/testing/#_5","title":"\u4f7f\u7528\u200b\u793a\u4f8b","text":""},{"location":"developer/testing/#jupyter-notebook","title":"Jupyter Notebook \u200b\u5feb\u901f\u200b\u5165\u95e8","text":"<ol> <li>\u200b\u542f\u52a8\u200bJupyter Notebook\uff1a</li> </ol> <pre><code>jupyter notebook examples/01_framework_quickstart.ipynb\n</code></pre> <ol> <li>\u200b\u6309\u200b\u987a\u5e8f\u200b\u6267\u884c\u200b\u6240\u6709\u200b\u5355\u5143\u683c\u200b\uff0c\u200b\u5b66\u4e60\u200b\u5982\u4f55\u200b\uff1a</li> <li>\u200b\u521d\u59cb\u5316\u200bBERT-CLAM\u200b\u6a21\u578b\u200b</li> <li>\u200b\u521b\u5efa\u200b\u6301\u7eed\u200b\u5b66\u4e60\u7b56\u7565\u200b</li> <li>\u200b\u8bad\u7ec3\u200b\u591a\u4e2a\u200b\u4efb\u52a1\u200b</li> <li>\u200b\u8bc4\u4f30\u200b\u6a21\u578b\u200b\u6027\u80fd\u200b</li> </ol>"},{"location":"developer/testing/#python","title":"Python\u200b\u811a\u672c\u200b\u6d4b\u8bd5","text":"<p>\u200b\u8fd0\u884c\u200b\u793a\u4f8b\u200b\u6d4b\u8bd5\u200b\u811a\u672c\u200b\u9a8c\u8bc1\u200b\u6846\u67b6\u200b\u529f\u80fd\u200b\uff1a</p> <pre><code>python examples/test_notebook.py\n</code></pre> <p>\u200b\u8be5\u200b\u811a\u672c\u200b\u4f1a\u200b\uff1a - \u200b\u521b\u5efa\u200b\u865a\u62df\u200b\u6570\u636e\u200b - \u200b\u521d\u59cb\u5316\u200b\u6a21\u578b\u200b\u548c\u200b\u7b56\u7565\u200b - \u200b\u6267\u884c\u200b\u5c0f\u89c4\u6a21\u200b\u8bad\u7ec3\u200b - \u200b\u9a8c\u8bc1\u200b\u63a8\u7406\u200b\u529f\u80fd\u200b</p>"},{"location":"developer/testing/#_6","title":"\u6d4b\u8bd5\u200b\u7ed3\u679c","text":""},{"location":"developer/testing/#_7","title":"\u5355\u5143\u6d4b\u8bd5\u200b\u8f93\u51fa\u200b\u793a\u4f8b","text":"<pre><code>..........\n----------------------------------------------------------------------\nRan 10 tests in 0.223s\n\nOK\n</code></pre>"},{"location":"developer/testing/#_8","title":"\u793a\u4f8b\u200b\u811a\u672c\u200b\u8f93\u51fa","text":"<pre><code>=== \u200b\u6d4b\u8bd5\u200bNotebook\u200b\u4ee3\u7801\u200b ===\n\n1. \u200b\u521d\u59cb\u5316\u200btokenizer\u200b\u548c\u200b\u8bbe\u5907\u200b...\n   \u200b\u8bbe\u5907\u200b: cpu\n\n2. \u200b\u521b\u5efa\u200b\u865a\u62df\u200b\u6570\u636e\u200b...\n   \u200b\u6570\u636e\u200b\u6279\u6b21\u200b: 2\n\n3. \u200b\u521d\u59cb\u5316\u200b\u6838\u5fc3\u200b\u6a21\u5757\u200b...\n   \u2713 \u200b\u6a21\u5757\u200b\u521d\u59cb\u5316\u200b\u5b8c\u6210\u200b\n\n4. \u200b\u521b\u5efa\u200b\u6a21\u578b\u200b\u548c\u200b\u7b56\u7565\u200b...\n   \u2713 \u200b\u7b56\u7565\u200b\u6570\u91cf\u200b: 3\n\n5. \u200b\u6267\u884c\u200b\u8bad\u7ec3\u200b\u6d4b\u8bd5\u200b...\n   \u200b\u6b65\u9aa4\u200b 1: \u200b\u635f\u5931\u200b=1.3397\n   \u200b\u6b65\u9aa4\u200b 2: \u200b\u635f\u5931\u200b=1.2980\n\n6. \u200b\u6267\u884c\u200b\u63a8\u7406\u200b\u6d4b\u8bd5\u200b...\n   \u200b\u9884\u6d4b\u200b\u5f62\u72b6\u200b: torch.Size([8])\n   \u200b\u9884\u6d4b\u503c\u200b: [1, 1, 1, 1]\n\n\u2713 \u200b\u6240\u6709\u200b\u6d4b\u8bd5\u901a\u8fc7\u200b\uff01Notebook\u200b\u4ee3\u7801\u200b\u53ef\u200b\u6b63\u5e38\u200b\u8fd0\u884c\u200b\u3002\n</code></pre>"},{"location":"developer/testing/#_9","title":"\u6545\u969c\u200b\u6392\u9664","text":""},{"location":"developer/testing/#_10","title":"\u5e38\u89c1\u95ee\u9898","text":"<ol> <li>ModuleNotFoundError: No module named 'bert_clam'</li> </ol> <p>\u200b\u89e3\u51b3\u65b9\u6848\u200b\uff1a\u200b\u5b89\u88c5\u5305\u200b    <code>bash    pip install -e .</code></p> <ol> <li>ImportError: faiss is required</li> </ol> <p>\u200b\u89e3\u51b3\u65b9\u6848\u200b\uff1a\u200b\u5b89\u88c5\u200bFAISS    <code>bash    pip install faiss-cpu  # CPU\u200b\u7248\u672c\u200b    # \u200b\u6216\u200b    pip install faiss-gpu  # GPU\u200b\u7248\u672c\u200b</code></p> <ol> <li>CUDA out of memory</li> </ol> <p>\u200b\u89e3\u51b3\u65b9\u6848\u200b\uff1a\u200b\u51cf\u5c0f\u200b\u6279\u6b21\u200b\u5927\u5c0f\u200b\u6216\u200b\u4f7f\u7528\u200bCPU    <code>python    device = torch.device('cpu')</code></p>"},{"location":"developer/testing/#_11","title":"\u4e0b\u200b\u4e00\u6b65","text":"<ul> <li>\u200b\u67e5\u770b\u200b README.md \u200b\u4e86\u89e3\u200b\u5b8c\u6574\u200b\u6587\u6863\u200b</li> <li>\u200b\u63a2\u7d22\u200b configs/ \u200b\u76ee\u5f55\u200b\u4e2d\u200b\u7684\u200b\u914d\u7f6e\u200b\u793a\u4f8b\u200b</li> <li>\u200b\u5728\u200b\u771f\u5b9e\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u6d4b\u8bd5\u200b\u6846\u67b6\u200b</li> <li>\u200b\u81ea\u5b9a\u4e49\u200b\u60a8\u200b\u81ea\u5df1\u200b\u7684\u200b\u6301\u7eed\u200b\u5b66\u4e60\u7b56\u7565\u200b</li> </ul>"},{"location":"developer/testing/#_12","title":"\u8d21\u732e","text":"<p>\u200b\u5982\u679c\u200b\u60a8\u200b\u53d1\u73b0\u200bbug\u200b\u6216\u200b\u6709\u200b\u6539\u8fdb\u200b\u5efa\u8bae\u200b\uff0c\u200b\u8bf7\u200b\uff1a 1. \u200b\u6dfb\u52a0\u200b\u76f8\u5e94\u200b\u7684\u200b\u5355\u5143\u6d4b\u8bd5\u200b 2. \u200b\u786e\u4fdd\u200b\u6240\u6709\u200b\u73b0\u6709\u200b\u6d4b\u8bd5\u901a\u8fc7\u200b 3. \u200b\u63d0\u4ea4\u200bPull Request</p> <p>\u200b\u9879\u76ee\u200b\u72b6\u6001\u200b: \u2705 \u200b\u6240\u6709\u200b\u6838\u5fc3\u200b\u529f\u80fd\u200b\u5df2\u200b\u6d4b\u8bd5\u200b\u5e76\u200b\u9a8c\u8bc1\u200b</p>"},{"location":"getting-started/installation/","title":"BERT-CLAM \u200b\u5feb\u901f\u200b\u5b89\u88c5\u200b\u6307\u5357","text":""},{"location":"getting-started/installation/#_1","title":"\u524d\u63d0\u6761\u4ef6","text":"<ul> <li>\u200b\u5df2\u200b\u521b\u5efa\u200b\u540d\u4e3a\u200b <code>bert_clam</code> \u200b\u7684\u200b conda \u200b\u73af\u5883\u200b\uff08Python 3.11\uff09</li> <li>\u200b\u5df2\u200b\u6fc0\u6d3b\u200b\u8be5\u200b\u73af\u5883\u200b</li> </ul>"},{"location":"getting-started/installation/#_2","title":"\u5b89\u88c5\u200b\u6b65\u9aa4","text":""},{"location":"getting-started/installation/#1","title":"\u65b9\u6cd5\u200b1\uff1a\u200b\u4f7f\u7528\u200b\u81ea\u52a8\u5316\u200b\u811a\u672c\u200b\uff08\u200b\u63a8\u8350\u200b\uff09","text":"<p>\u200b\u5728\u200b <code>bert_clam_library</code> \u200b\u76ee\u5f55\u200b\u4e0b\u200b\u8fd0\u884c\u200b\uff1a</p> <pre><code># Windows\nsetup_and_test.bat\n\n# Linux/Mac\nbash setup_and_test.sh\n</code></pre> <p>\u200b\u811a\u672c\u200b\u4f1a\u200b\u81ea\u52a8\u200b\u5b8c\u6210\u200b\u4ee5\u4e0b\u200b\u64cd\u4f5c\u200b\uff1a 1. \u200b\u5b89\u88c5\u200b PyTorch 2.1.0 (CPU\u200b\u7248\u672c\u200b) 2. \u200b\u5b89\u88c5\u200b Transformers 4.35.0 3. \u200b\u5b89\u88c5\u200b\u5176\u4ed6\u200b\u4f9d\u8d56\u200b 4. \u200b\u5b89\u88c5\u200b BERT-CLAM \u200b\u5e93\u200b 5. \u200b\u9a8c\u8bc1\u200b\u5b89\u88c5\u200b 6. \u200b\u8fd0\u884c\u200b\u5b8c\u6574\u200b\u6d4b\u8bd5\u200b</p>"},{"location":"getting-started/installation/#2","title":"\u65b9\u6cd5\u200b2\uff1a\u200b\u624b\u52a8\u200b\u5b89\u88c5","text":"<pre><code># 1. \u200b\u6fc0\u6d3b\u200b\u73af\u5883\u200b\nconda activate bert_clam\n\n# 2. \u200b\u8fdb\u5165\u200b\u5e93\u200b\u76ee\u5f55\u200b\ncd bert_clam_library\n\n# 3. \u200b\u5b89\u88c5\u200b PyTorch (CPU\u200b\u7248\u672c\u200b)\npip install torch==2.1.0 torchvision==0.16.0 --index-url https://download.pytorch.org/whl/cpu\n\n# 4. \u200b\u5b89\u88c5\u200b Transformers\npip install transformers==4.35.0\n\n# 5. \u200b\u5b89\u88c5\u200b\u5176\u4ed6\u200b\u4f9d\u8d56\u200b\npip install -r requirements.txt\n\n# 6. \u200b\u5b89\u88c5\u200b BERT-CLAM \u200b\u5e93\u200b\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#_3","title":"\u9a8c\u8bc1\u200b\u5b89\u88c5","text":"<p>\u200b\u5b89\u88c5\u200b\u5b8c\u6210\u200b\u540e\u200b\uff0c\u200b\u8fd0\u884c\u200b\u4ee5\u4e0b\u200b\u547d\u4ee4\u200b\u9a8c\u8bc1\u200b\uff1a</p> <pre><code># \u200b\u6d4b\u8bd5\u200b\u57fa\u672c\u200b\u5bfc\u5165\u200b\npython -c \"import bert_clam; print('BERT-CLAM \u200b\u7248\u672c\u200b:', bert_clam.__version__)\"\npython -c \"from bert_clam.training import BERTCLAMTrainer; print('\u2713 BERTCLAMTrainer \u200b\u5bfc\u5165\u200b\u6210\u529f\u200b')\"\n\n# \u200b\u8fd0\u884c\u200b\u5b8c\u6574\u200b\u6d4b\u8bd5\u200b\npython complete_test.py\n</code></pre>"},{"location":"getting-started/installation/#_4","title":"\u9884\u671f\u200b\u7ed3\u679c","text":"<p>\u200b\u5b8c\u6574\u200b\u6d4b\u8bd5\u200b\u5e94\u8be5\u200b\u663e\u793a\u200b\uff1a</p> <pre><code>\u2713 bert_clam \u200b\u5305\u200b\u5bfc\u5165\u200b: PASS\n\u2713 BERTCLAMTrainer \u200b\u5bfc\u5165\u200b: PASS\n\u2713 config_loader \u200b\u6a21\u5757\u200b\u5bfc\u5165\u200b: PASS\n\u2713 lora_adapter \u200b\u6a21\u5757\u200b\u5bfc\u5165\u200b: PASS\n\u2713 BERT-CLA\u200b\u6a21\u578b\u200b: PASS\n\u2713 COLA\u200b\u914d\u7f6e\u200b: PASS\n\u2713 \u200b\u6570\u636e\u5904\u7406\u200b: PASS\n\u2713 \u200b\u6a21\u578b\u200b\u524d\u5411\u200b\u4f20\u64ad\u200b: PASS\n\u2713 \u200b\u8bad\u7ec3\u200b\u6b65\u9aa4\u200b: PASS\n\u2713 \u200b\u8bc4\u4f30\u200b\u6307\u6807\u200b: PASS\n\n\u200b\u6d4b\u8bd5\u200b\u7ed3\u679c\u200b: 10/10 \u200b\u4e2a\u200b\u6d4b\u8bd5\u901a\u8fc7\u200b\n\u200b\u6210\u529f\u7387\u200b: 100.0%\n</code></pre>"},{"location":"getting-started/installation/#_5","title":"\u6545\u969c\u200b\u6392\u9664","text":""},{"location":"getting-started/installation/#1pytorch","title":"\u95ee\u9898\u200b1\uff1aPyTorch \u200b\u5b89\u88c5\u200b\u5931\u8d25","text":"<pre><code># \u200b\u5c1d\u8bd5\u200b\u4f7f\u7528\u200b conda \u200b\u5b89\u88c5\u200b\nconda install pytorch==2.1.0 torchvision==0.16.0 cpuonly -c pytorch\n</code></pre>"},{"location":"getting-started/installation/#2transformers","title":"\u95ee\u9898\u200b2\uff1aTransformers \u200b\u5bfc\u5165\u200b\u9519\u8bef","text":"<pre><code># \u200b\u91cd\u65b0\u5b89\u88c5\u200b transformers\npip uninstall transformers -y\npip install transformers==4.35.0\n</code></pre>"},{"location":"getting-started/installation/#3bert-clam","title":"\u95ee\u9898\u200b3\uff1aBERT-CLAM \u200b\u5bfc\u5165\u200b\u5931\u8d25","text":"<pre><code># \u200b\u91cd\u65b0\u5b89\u88c5\u200b\u5e93\u200b\npip uninstall bert-clam -y\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#_6","title":"\u73af\u5883\u200b\u4fe1\u606f","text":"<p>\u200b\u63a8\u8350\u200b\u7684\u200b\u73af\u5883\u200b\u914d\u7f6e\u200b\uff1a</p> <pre><code>Python: 3.11\nPyTorch: 2.1.0\ntorchvision: 0.16.0\ntransformers: 4.35.0\nnumpy: 1.24.3\npandas: 2.0.3\nscikit-learn: 1.3.0\n</code></pre>"},{"location":"getting-started/installation/#_7","title":"\u4e0b\u200b\u4e00\u6b65","text":"<p>\u200b\u5b89\u88c5\u200b\u6210\u529f\u200b\u540e\u200b\uff0c\u200b\u60a8\u200b\u53ef\u4ee5\u200b\uff1a</p> <ol> <li>\u200b\u67e5\u770b\u200b README.md \u200b\u4e86\u89e3\u200b\u4f7f\u7528\u200b\u65b9\u6cd5\u200b</li> <li>\u200b\u8fd0\u884c\u200b\u793a\u4f8b\u200b\u4ee3\u7801\u200b\u6d4b\u8bd5\u200b\u529f\u80fd\u200b</li> <li>\u200b\u5f00\u59cb\u200b\u60a8\u200b\u7684\u200b\u6301\u7eed\u200b\u5b66\u4e60\u200b\u5b9e\u9a8c\u200b</li> </ol>"},{"location":"getting-started/installation/#_8","title":"\u83b7\u53d6\u200b\u5e2e\u52a9","text":"<p>\u200b\u5982\u679c\u200b\u9047\u5230\u200b\u95ee\u9898\u200b\uff0c\u200b\u8bf7\u200b\u67e5\u770b\u200b\uff1a - ENVIRONMENT_COMPATIBILITY_REPORT.md - \u200b\u73af\u5883\u200b\u517c\u5bb9\u6027\u200b\u8be6\u7ec6\u4fe1\u606f\u200b - README.md - \u200b\u5b8c\u6574\u200b\u6587\u6863\u200b</p>"},{"location":"getting-started/quick-start/","title":"Quick Start","text":"<p>This guide will help you get started with the BERT-CLAM framework quickly.</p>"},{"location":"getting-started/quick-start/#installation","title":"Installation","text":"<p>First, install the framework:</p> <pre><code>conda create -n bert_clam python=3.11 -y\nconda activate bert_clam\npip install -r requirements-lock.txt\npip install -e .\n</code></pre>"},{"location":"getting-started/quick-start/#minimal-example","title":"Minimal Example","text":"<p>Here's a minimal example to get you started:</p> <pre><code>import torch\nfrom transformers import AutoTokenizer\nfrom bert_clam.models.bert_clam_model import BERTCLAMModel\n\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\nmodel = BERTCLAMModel(\n    model_name='bert-base-uncased',\n    num_labels=2,\n    enable_grammar=True,\n    enable_ewc=True\n)\n\ninputs = tokenizer(\"Example sentence\", return_tensors=\"pt\")\noutputs = model(**inputs, task_id=0)\nprint(f\"Logits: {outputs['logits']}\")\n</code></pre>"},{"location":"getting-started/quick-start/#run-a-full-experiment","title":"Run a Full Experiment","text":"<p>To run a complete experiment with predefined configurations:</p> <pre><code>python run_experiment.py --config configs/ablation_full_model.json\n</code></pre>"},{"location":"getting-started/quick-start/#configuration-driven-experiments","title":"Configuration-Driven Experiments","text":"<p>All experiments in BERT-CLAM are driven by JSON configuration files. You can run any experiment with a single command:</p> <pre><code>python run_experiment.py --config configs/example_strategy_config.json\n</code></pre> <p>The framework comes with several example configurations in the <code>configs/</code> directory:</p> <ul> <li><code>ablation_baseline.json</code>: No continual learning</li> <li><code>ablation_ewc_only.json</code>: EWC only</li> <li><code>ablation_full_model.json</code>: All strategies combined</li> <li><code>example_strategy_config.json</code>: Custom strategy ordering</li> </ul>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<p>Now that you've run your first example, you can:</p> <ol> <li>Explore the Core Concepts to understand the framework architecture</li> <li>Try the End-to-End Example tutorial</li> <li>Experiment with different continual learning strategies</li> <li>Create your own custom strategies following the Strategy Pattern guide</li> </ol>"},{"location":"tutorials/notebook_guide/","title":"End-to-End Example","text":"<p>This tutorial demonstrates how to use the BERT-CLAM framework for continual learning tasks using our example notebook.</p>"},{"location":"tutorials/notebook_guide/#overview","title":"Overview","text":"<p>The example notebook <code>01_framework_quickstart.ipynb</code> in the <code>examples/</code> directory provides a complete walkthrough of using BERT-CLAM for a continual learning scenario. The notebook covers:</p> <ol> <li>Environment setup</li> <li>Data loading</li> <li>Model and module initialization</li> <li>Strategy creation and composition</li> <li>Training with injected strategies</li> <li>Evaluation and results summary</li> </ol>"},{"location":"tutorials/notebook_guide/#running-the-example","title":"Running the Example","text":""},{"location":"tutorials/notebook_guide/#prerequisites","title":"Prerequisites","text":"<p>Make sure you have installed the framework following the Installation guide.</p>"},{"location":"tutorials/notebook_guide/#launch-the-notebook","title":"Launch the Notebook","text":"<p>You can run the example notebook in one of these ways:</p> <p>Option 1: Jupyter Notebook</p> <pre><code>jupyter notebook examples/01_framework_quickstart.ipynb\n</code></pre> <p>Option 2: Jupyter Lab</p> <pre><code>jupyter lab examples/01_framework_quickstart.ipynb\n</code></pre>"},{"location":"tutorials/notebook_guide/#key-concepts-demonstrated","title":"Key Concepts Demonstrated","text":""},{"location":"tutorials/notebook_guide/#1-component-initialization","title":"1. Component Initialization","text":"<p>The notebook demonstrates how to initialize the core continual learning modules:</p> <pre><code>from bert_clam.core.ewc import EnhancedElasticWeightConsolidation\nfrom bert_clam.core.memory_replay_bank import EnhancedAdaptiveMemoryRetrieval\nfrom bert_clam.core.grammar_aware import EnhancedGrammarAwareModule\n\newc_module = EnhancedElasticWeightConsolidation(\n    lambda_ewc=0.5,\n    fisher_samples=10\n)\n\nmrb_module = EnhancedAdaptiveMemoryRetrieval(\n    hidden_size=768,\n    k=5,\n    memory_dim=768\n)\n\ngrammar_module = EnhancedGrammarAwareModule(\n    hidden_size=768,\n    num_attention_heads=12,\n    grammar_features_dim=64\n)\n</code></pre>"},{"location":"tutorials/notebook_guide/#2-strategy-composition","title":"2. Strategy Composition","text":"<p>The notebook shows how to create and compose strategies:</p> <pre><code>from bert_clam.core.strategy import EWCStrategy, MRBStrategy, GrammarStrategy\n\nstrategies = [\n    GrammarStrategy(grammar_module.to(device)),\n    MRBStrategy(mrb_module.to(device), fusion_weight=0.2),\n    EWCStrategy(ewc_module, model)\n]\n\n# Inject strategies into the model\nmodel.strategies = strategies\n</code></pre>"},{"location":"tutorials/notebook_guide/#3-multi-task-training","title":"3. Multi-Task Training","text":"<p>The notebook demonstrates training on multiple tasks sequentially:</p> <pre><code>def train_task(model, dataloader, task_id, num_steps=5):\n    \"\"\"Train a single task\"\"\"\n    model.train()\n    model.register_task(task_id)\n\n    # ... training loop implementation\n</code></pre>"},{"location":"tutorials/notebook_guide/#understanding-the-code-structure","title":"Understanding the Code Structure","text":"<p>The notebook is organized into these sections:</p> <ol> <li>Environment Setup: Installing dependencies and importing modules</li> <li>Data Loading: Creating dummy data for demonstration (replace with your own dataset)</li> <li>Model Initialization: Setting up the BERT-CLAM model and core modules</li> <li>Strategy Composition: Creating and combining different continual learning strategies</li> <li>Training: Implementing the training loop for multiple tasks</li> <li>Evaluation: Assessing model performance across tasks</li> <li>Summary: Key takeaways and next steps</li> </ol>"},{"location":"tutorials/notebook_guide/#adapting-for-your-use-case","title":"Adapting for Your Use Case","text":"<p>To use the notebook with your own data:</p> <ol> <li>Replace the <code>create_dummy_data()</code> function with your data loading logic</li> <li>Adjust the <code>num_labels</code> parameter based on your classification task</li> <li>Modify the training parameters (learning rate, epochs, etc.) as needed</li> <li>Update the evaluation metrics based on your specific requirements</li> </ol>"},{"location":"tutorials/notebook_guide/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>BERT-CLAM allows flexible combination of continual learning strategies</li> <li>The strategy pattern enables modular design and easy experimentation</li> <li>Multiple tasks can be trained sequentially with minimal code changes</li> <li>The framework handles task memory and strategy application automatically</li> </ul>"},{"location":"tutorials/notebook_guide/#next-steps","title":"Next Steps","text":"<p>After completing the tutorial, you can:</p> <ol> <li>Experiment with different combinations of strategies</li> <li>Apply the framework to your own datasets</li> <li>Create custom strategies following the pattern shown in the Strategy Pattern guide</li> <li>Explore the configuration files in the <code>configs/</code> directory for more advanced usage</li> </ol>"}]}