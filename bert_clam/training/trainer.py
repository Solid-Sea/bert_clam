"""
BERT-CLAMè®­ç»ƒå™¨
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from typing import Dict, Optional, Any, List
import os
import json
from tqdm import tqdm
import numpy as np
import logging
import traceback
import wandb

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('bert_clam_training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ä¿®æ­£å¯¼å…¥è·¯å¾„
from bert_clam.models.bert_clam_model import BERTCLAMModel
from bert_clam.data.continual_dataset import ContinualDataset
from bert_clam.evaluation.forgetting_evaluator import ForgettingEvaluator  # ä¿®æ­£å¯¼å…¥


class BERTCLAMTrainer:
    """BERT-CLAMè®­ç»ƒå™¨"""
    
    def __init__(self,
                 model: BERTCLAMModel,
                 config: Dict[str, Any],
                 output_dir: str = "./experiments",
                 use_wandb: bool = False,
                 wandb_project: str = "bert-clam",
                 wandb_run_name: str = None):
        self.model = model
        self.config = config
        self.output_dir = output_dir
        self.use_wandb = use_wandb
        
        if self.use_wandb:
            wandb.init(project=wandb_project, name=wandb_run_name, config=config)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        os.makedirs(f"{output_dir}/checkpoints", exist_ok=True)
        os.makedirs(f"{output_dir}/logs", exist_ok=True)
        
        # ä¼˜åŒ–å™¨
        # ä¼˜åŒ–å™¨ - åªä¼˜åŒ–å¯è®­ç»ƒçš„å‚æ•°
        trainable_params = [p for p in model.parameters() if p.requires_grad]
        print(f"ä¼˜åŒ–å™¨æ‰¾åˆ°äº† {len(trainable_params)} ä¸ªå¯è®­ç»ƒçš„å‚æ•°å¼ é‡ã€‚")
        self.optimizer = torch.optim.AdamW(
            trainable_params,
            lr=config.get('learning_rate', 2e-5),
            weight_decay=config.get('weight_decay', 0.01)
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨ - æ ¹æ®å®é™…è®­ç»ƒæ­¥æ•°åŠ¨æ€è®¡ç®—
        from transformers import get_linear_schedule_with_warmup
        # å¦‚æœé…ç½®ä¸­æ²¡æœ‰æŒ‡å®šï¼Œåˆ™ä¸åˆ›å»ºè°ƒåº¦å™¨ï¼ˆä½¿ç”¨å¸¸æ•°å­¦ä¹ ç‡ï¼‰
        if 'warmup_steps' in config or 'num_training_steps' in config:
            self.scheduler = get_linear_schedule_with_warmup(
                self.optimizer,
                num_warmup_steps=config.get('warmup_steps', 0),
                num_training_steps=config.get('num_training_steps', 1000)
            )
        else:
            self.scheduler = None
        
        # åˆå§‹åŒ–é—å¿˜è¯„ä¼°å™¨
        self.forgetting_evaluator = ForgettingEvaluator()
        
        # è®­ç»ƒå†å²
        self.training_history = {
            'task_losses': {},
            'task_accuracies': {},
            'forgetting_rates': {},
            'backward_transfers': {}
        }
        
        # å½“å‰ä»»åŠ¡ä¿¡æ¯
        self.current_task_id = 0
        self.task_performance_history = {}
        self.task_names = []  # å­˜å‚¨ä»»åŠ¡åç§°

        # ç¦ç”¨æ··åˆç²¾åº¦è®­ç»ƒä»¥é¿å…å…¼å®¹æ€§é—®é¢˜
        self.use_amp = False
        self.scaler = None
    
    def train_task(self,
                   task_id: int,
                   train_loader: DataLoader,
                   val_loader: DataLoader = None,
                   epochs: int = 3,
                   save_checkpoint: bool = True,
                   early_stopping_patience: int = None,
                   early_stopping_min_delta: float = 0.001):
        """è®­ç»ƒå•ä¸ªä»»åŠ¡
        
        Args:
            early_stopping_patience: æ—©åœè€å¿ƒå€¼,è¿ç»­å¤šå°‘ä¸ªepochéªŒè¯é›†æ€§èƒ½æ— æ”¹å–„åˆ™åœæ­¢
            early_stopping_min_delta: æœ€å°æ”¹å–„é˜ˆå€¼
        """
        logger.info(f"=" * 60)
        logger.info(f"å¼€å§‹è®­ç»ƒä»»åŠ¡ {task_id}")
        logger.info(f"è®­ç»ƒè½®æ•°: {epochs}, æ‰¹æ¬¡å¤§å°: {train_loader.batch_size}")
        logger.info(f"=" * 60)
        
        try:
            self.model.train()
            
            # æ³¨å†Œæ–°ä»»åŠ¡
            logger.info(f"æ³¨å†Œä»»åŠ¡ {task_id}...")
            self.model.register_task(task_id)
            logger.info(f"ä»»åŠ¡ {task_id} æ³¨å†ŒæˆåŠŸ")
        except Exception as e:
            logger.error(f"ä»»åŠ¡æ³¨å†Œå¤±è´¥: {str(e)}")
            logger.error(traceback.format_exc())
            raise
        
        # æ—©åœç›¸å…³å˜é‡
        best_val_acc = 0.0
        epochs_without_improvement = 0
        
        for epoch in range(epochs):
            self.model.train()  # ç¡®ä¿æ¯ä¸ªepochå¼€å§‹æ—¶æ¨¡å‹å¤„äºè®­ç»ƒæ¨¡å¼
            total_loss = 0
            num_batches = 0
            
            logger.info(f"Epoch {epoch+1}/{epochs} å¼€å§‹")
            progress_bar = tqdm(train_loader, desc=f"Task {task_id}, Epoch {epoch+1}/{epochs}")
            
            for batch_idx, batch in enumerate(progress_bar):
                try:
                    input_ids = batch['input_ids'].to(self.model.device)
                    attention_mask = batch['attention_mask'].to(self.model.device)
                    # å°è¯•è·å–æ ‡ç­¾ï¼Œæ”¯æŒ 'labels' å’Œ 'label' ä¸¤ç§æ ¼å¼
                    if 'labels' in batch:
                        labels = batch['labels'].to(self.model.device)
                    elif 'label' in batch:
                        labels = batch['label'].to(self.model.device)
                    else:
                        raise KeyError("Batch does not contain 'labels' or 'label' key")
                    
                    # ç¡®ä¿æ ‡ç­¾ç»´åº¦æ­£ç¡®
                    if labels.dim() == 2 and labels.size(-1) == self.model.num_labels:
                        labels = torch.argmax(labels, dim=-1)
                    elif labels.dim() > 1:
                        labels = labels.squeeze(-1)
                    
                    labels = labels.long()
                    labels = torch.clamp(labels, 0, self.model.num_labels - 1)
                    
                    # å‰å‘ä¼ æ’­(ä¸ä½¿ç”¨æ··åˆç²¾åº¦)
                    outputs = self.model(
                        input_ids=input_ids,
                        attention_mask=attention_mask,
                        labels=labels,
                        task_id=task_id
                    )
                    
                    if 'loss' in outputs:
                        loss = outputs['loss']
                    else:
                        logits = outputs['logits']
                        loss = nn.functional.cross_entropy(logits, labels)
                    
                    # åå‘ä¼ æ’­
                    self.optimizer.zero_grad()
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(),
                                                 self.config.get('gradient_clip', 1.0))
                    self.optimizer.step()
                    
                    if self.scheduler:
                        self.scheduler.step()
                    
                    total_loss += loss.item()
                    num_batches += 1
                    
                    # æ›´æ–°è¿›åº¦æ¡
                    progress_bar.set_postfix({
                        'loss': f'{loss.item():.4f}',
                        'lr': f'{self.optimizer.param_groups[0]["lr"]:.2e}'
                    })
                    
                    if self.use_wandb:
                        wandb.log({
                            f'task_{task_id}/loss': loss.item(),
                            f'task_{task_id}/lr': self.optimizer.param_groups[0]["lr"]
                        })
                    
                    # æ›´æ–°è®°å¿†åº“
                    with torch.no_grad():
                        self.model.update_memory(
                            input_ids, attention_mask, labels, task_id
                        )
                    
                    # æ¯100ä¸ªæ‰¹æ¬¡è®°å½•ä¸€æ¬¡è¯¦ç»†ä¿¡æ¯
                    if batch_idx % 100 == 0:
                        logger.info(f"Batch {batch_idx}: loss={loss.item():.4f}")
                        
                except Exception as e:
                    logger.error(f"æ‰¹æ¬¡ {batch_idx} è®­ç»ƒå¤±è´¥: {str(e)}")
                    logger.error(traceback.format_exc())
                    raise
            
            avg_loss = total_loss / num_batches
            logger.info(f"Task {task_id}, Epoch {epoch+1}, Average Loss: {avg_loss:.4f}")
            
            if self.use_wandb:
                wandb.log({f'task_{task_id}/epoch_loss': avg_loss, 'epoch': epoch})
            
            # éªŒè¯ï¼ˆå¦‚æœæä¾›éªŒè¯æ•°æ®ï¼‰
            # æ³¨æ„: è¿™é‡Œçš„éªŒè¯åªæ˜¯è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç›‘æ§,ä¸è®°å½•åˆ°æ€§èƒ½å†å²
            if val_loader:
                try:
                    val_acc = self.evaluate(val_loader, task_id, record_performance=False)
                    logger.info(f"Task {task_id}, Validation Accuracy: {val_acc:.4f}")
                    if self.use_wandb:
                        wandb.log({f'task_{task_id}/val_accuracy': val_acc, 'epoch': epoch})
                    
                    # æ—©åœæ£€æŸ¥
                    if early_stopping_patience is not None:
                        if val_acc > best_val_acc + early_stopping_min_delta:
                            best_val_acc = val_acc
                            epochs_without_improvement = 0
                            logger.info(f"éªŒè¯å‡†ç¡®ç‡æå‡åˆ° {val_acc:.4f}")
                        else:
                            epochs_without_improvement += 1
                            logger.info(f"éªŒè¯å‡†ç¡®ç‡æ— æ”¹å–„ ({epochs_without_improvement}/{early_stopping_patience})")
                            
                            if epochs_without_improvement >= early_stopping_patience:
                                logger.info(f"æ—©åœè§¦å‘ï¼è¿ç»­ {early_stopping_patience} ä¸ªepochæ— æ”¹å–„")
                                break
                except Exception as e:
                    logger.error(f"éªŒè¯å¤±è´¥: {str(e)}")
                    logger.error(traceback.format_exc())
        
        # ä¿å­˜ä»»åŠ¡æ£€æŸ¥ç‚¹ï¼ˆç”¨äºEWCï¼‰
        try:
            if hasattr(self.model, 'save_task_checkpoint'):
                logger.info(f"ä¿å­˜ä»»åŠ¡ {task_id} çš„EWCæ£€æŸ¥ç‚¹...")
                self.model.save_task_checkpoint(task_id, train_loader)
        except Exception as e:
            logger.warning(f"EWCæ£€æŸ¥ç‚¹ä¿å­˜å¤±è´¥: {str(e)}")
        
        # è®°å½•ä»»åŠ¡æŸå¤±
        self.training_history['task_losses'].setdefault(task_id, []).append(avg_loss)
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        if save_checkpoint:
            try:
                self.save_checkpoint(task_id)
                logger.info(f"ä»»åŠ¡ {task_id} è®­ç»ƒå®Œæˆå¹¶ä¿å­˜æ£€æŸ¥ç‚¹")
            except Exception as e:
                logger.error(f"æ£€æŸ¥ç‚¹ä¿å­˜å¤±è´¥: {str(e)}")
                logger.error(traceback.format_exc())
    
    def evaluate(self,
                data_loader: DataLoader,
                task_id: int = 0,
                compute_forgetting: bool = False,
                record_performance: bool = True) -> float:
        """è¯„ä¼°æ¨¡å‹
        
        Args:
            data_loader: æ•°æ®åŠ è½½å™¨
            task_id: ä»»åŠ¡ID
            compute_forgetting: æ˜¯å¦è®¡ç®—é—å¿˜ç‡(å·²åºŸå¼ƒ,ä¿ç•™ç”¨äºå…¼å®¹æ€§)
            record_performance: æ˜¯å¦å°†ç»“æœè®°å½•åˆ°æ€§èƒ½å†å²ä¸­
                               True: è®°å½•(ç”¨äºè®­ç»ƒåçš„éªŒè¯å’Œé—å¿˜ç‡è·Ÿè¸ª)
                               False: ä¸è®°å½•(ç”¨äºä¸´æ—¶è¯„ä¼°)
        """
        self.model.eval()
        correct = 0
        total = 0
        
        # ğŸ” è¯Šæ–­æ—¥å¿—ï¼šè®°å½•æ¨¡å‹åˆ†ç±»å™¨å‚æ•°çŠ¶æ€
        try:
            if hasattr(self.model.backbone.bert.classifier, 'weight'):
                classifier_params = self.model.backbone.bert.classifier.weight.data
            elif hasattr(self.model.backbone.bert.classifier, 'linear'):
                classifier_params = self.model.backbone.bert.classifier.linear.weight.data
            else:
                # LoRAæ³¨å…¥çš„æƒ…å†µï¼Œè·å–åŸºç¡€æƒé‡
                classifier_params = None
                for name, param in self.model.backbone.bert.classifier.named_parameters():
                    if 'weight' in name and param.requires_grad:
                        classifier_params = param.data
                        break
            
            if classifier_params is not None:
                logger.info(f"[DIAG] Eval Task {task_id} - Classifier params mean: {classifier_params.mean().item():.6f}, std: {classifier_params.std().item():.6f}")
        except Exception as e:
            logger.warning(f"[DIAG] Cannot get classifier params: {e}")
        
        with torch.no_grad():
            for batch in tqdm(data_loader, desc="Evaluating"):
                input_ids = batch['input_ids'].to(self.model.device)
                attention_mask = batch['attention_mask'].to(self.model.device)
                # å°è¯•è·å–æ ‡ç­¾ï¼Œæ”¯æŒ 'labels' å’Œ 'label' ä¸¤ç§æ ¼å¼
                if 'labels' in batch:
                    labels = batch['labels'].to(self.model.device)
                elif 'label' in batch:
                    labels = batch['label'].to(self.model.device)
                else:
                    raise KeyError("Batch does not contain 'labels' or 'label' key")
                
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    task_id=task_id
                )
                
                predictions = torch.argmax(outputs['logits'], dim=-1)
                correct += (predictions == labels).sum().item()
                total += labels.size(0)
        
        accuracy = correct / total
        
        # åªåœ¨éœ€è¦æ—¶è®°å½•ä»»åŠ¡æ€§èƒ½
        if record_performance:
            if task_id not in self.task_performance_history:
                self.task_performance_history[task_id] = []
            self.task_performance_history[task_id].append(accuracy)
            logger.info(f"[DIAG] Record Task {task_id} performance: {accuracy:.4f}, history length: {len(self.task_performance_history[task_id])}, full history: {self.task_performance_history[task_id]}")
            
            if self.use_wandb:
                wandb.log({f'eval/task_{task_id}_accuracy': accuracy})
        
        return accuracy
    
    def evaluate_all_tasks(self, 
                          task_loaders: Dict[int, DataLoader],
                          current_task_id: int) -> Dict[int, float]:
        """è¯„ä¼°æ‰€æœ‰å·²å­¦ä¹ çš„ä»»åŠ¡ï¼ˆç”¨äºè®¡ç®—é—å¿˜ç‡ï¼‰"""
        task_accuracies = {}
        
        for task_id, loader in task_loaders.items():
            if task_id <= current_task_id: # åªè¯„ä¼°å·²å­¦ä¹ çš„ä»»åŠ¡
                acc = self.evaluate(loader, task_id)
                task_accuracies[task_id] = acc
                print(f"Task {task_id} Accuracy: {acc:.4f}")
        
        return task_accuracies
    
    def compute_forgetting_rate(self, task_id: int) -> float:
        """è®¡ç®—ç‰¹å®šä»»åŠ¡çš„é—å¿˜ç‡"""
        if task_id not in self.task_performance_history:
            return 0.0
        
        performance_history = self.task_performance_history[task_id]
        if len(performance_history) < 2:
            return 0.0
        
        # è®¡ç®—æœ€é«˜å‡†ç¡®ç‡ï¼ˆé™¤äº†æœ€ç»ˆï¼‰å’Œæœ€ç»ˆå‡†ç¡®ç‡
        max_acc_before_final = max(performance_history[:-1])
        final_acc = performance_history[-1]
        
        forgetting_rate = max(0.0, max_acc_before_final - final_acc)
        return forgetting_rate
    
    def compute_backward_transfer(self, task_id: int) -> float:
        """è®¡ç®—å‘åè¿ç§»"""
        if task_id <= 0 or task_id not in self.task_performance_history:
            return 0.0
        
        # å½“å‰ä»»åŠ¡åœ¨å­¦ä¹ åçš„æ€§èƒ½
        current_task_final_acc = self.task_performance_history[task_id][-1] if self.task_performance_history[task_id] else 0.0
        
        # å‰ä¸€ä¸ªä»»åŠ¡çš„æ€§èƒ½å˜åŒ–
        prev_task_id = task_id - 1
        if prev_task_id in self.task_performance_history:
            prev_task_initial_acc = self.task_performance_history[prev_task_id][0] if len(self.task_performance_history[prev_task_id]) > 0 else 0.0
            prev_task_final_acc = self.task_performance_history[prev_task_id][-1] if self.task_performance_history[prev_task_id] else 0.0
            
            bwt = prev_task_final_acc - prev_task_initial_acc
            return bwt
        
        return 0.0
    
    def continual_learning_train(self,
                                continual_dataset: ContinualDataset,
                                epochs_per_task: int = 3):
        """æŒç»­å­¦ä¹ è®­ç»ƒæµç¨‹"""
        print("å¼€å§‹æŒç»­å­¦ä¹ è®­ç»ƒ...")
        
        task_loaders = {}
        
        # è·å–ä»»åŠ¡åç§°
        num_tasks = continual_dataset.get_num_tasks()
        for task_idx in range(num_tasks):
            task_info = continual_dataset.get_task_info(task_idx)  # å‡è®¾æœ‰è¿™ä¸ªæ–¹æ³•è·å–ä»»åŠ¡ä¿¡æ¯
            if task_info:
                task_name = task_info.get('name', f'task_{task_idx}')
                if task_name not in self.task_names:
                    self.task_names.append(task_name)
        
        # è®­ç»ƒæ¯ä¸ªä»»åŠ¡
        for task_idx in range(num_tasks):
            task_info = continual_dataset.get_next_task(self.config.get('batch_size', 32))
            if task_info is None:
                break
            
            task_name, train_loader, val_loader = task_info
            task_id = task_idx
            
            print(f"\n=== è®­ç»ƒä»»åŠ¡ {task_id}: {task_name} ===")
            
            # è®­ç»ƒå½“å‰ä»»åŠ¡
            self.train_task(
                task_id=task_id,
                train_loader=train_loader,
                val_loader=val_loader,
                epochs=epochs_per_task
            )
            
            # è¯„ä¼°æ‰€æœ‰å·²å­¦ä¹ çš„ä»»åŠ¡ï¼ˆç”¨äºè®¡ç®—é—å¿˜ç‡ï¼‰
            print(f"\nè¯„ä¼°æ‰€æœ‰å·²å­¦ä¹ çš„ä»»åŠ¡ä»¥è®¡ç®—é—å¿˜ç‡...")
            for eval_task_idx in range(task_id + 1):  # è¯„ä¼°ä»0åˆ°å½“å‰ä»»åŠ¡çš„æ‰€æœ‰ä»»åŠ¡
                eval_task_name = self.task_names[eval_task_idx] if eval_task_idx < len(self.task_names) else f"task_{eval_task_idx}"
                
                # è·å–è¯„ä¼°æ•°æ®åŠ è½½å™¨
                eval_task_info = continual_dataset.get_task_by_id(eval_task_idx, split='validation')
                if eval_task_info:
                    _, eval_loader, _ = eval_task_info
                    eval_acc = self.evaluate(eval_loader, eval_task_idx)
                    
                    # è®°å½•æ€§èƒ½åˆ°é—å¿˜è¯„ä¼°å™¨
                    stage_name = f"after_task_{task_id}"
                    self.forgetting_evaluator.record_task_performance_by_name(
                        eval_task_name, stage_name, eval_acc
                    )
                    
                    print(f"  {eval_task_name} åœ¨é˜¶æ®µ '{stage_name}' çš„å‡†ç¡®ç‡: {eval_acc:.4f}")
            
            # è®¡ç®—å½“å‰çš„é—å¿˜æŒ‡æ ‡
            current_forgetting = self.forgetting_evaluator.compute_average_forgetting()
            current_bwt = self.forgetting_evaluator.compute_backward_transfer(self.task_names[:task_id+1])
            
            print(f"å½“å‰å¹³å‡é—å¿˜ç‡: {current_forgetting:.4f}")
            print(f"å½“å‰å‘åè¿ç§»: {current_bwt:.4f}")
        
        print("\næŒç»­å­¦ä¹ è®­ç»ƒå®Œæˆï¼")
        
        # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
        final_metrics = self.forgetting_evaluator.get_comprehensive_metrics(self.task_names)
        
        # ä¿å­˜è®­ç»ƒå†å²
        self.save_training_history()
        
        return final_metrics
    
    def save_checkpoint(self, task_id: int):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_path = f"{self.output_dir}/checkpoints/model_task_{task_id}.pt"
        
        # ğŸ” è¯Šæ–­ï¼šè®°å½•ä¿å­˜æ—¶çš„æ¨¡å‹çŠ¶æ€
        if hasattr(self.model.backbone.bert.classifier, 'weight'):
            classifier_weight = self.model.backbone.bert.classifier.weight.data
            weight_hash = hash(classifier_weight.cpu().numpy().tobytes())
            weight_mean = classifier_weight.mean().item()
            logger.info(f"[DEBUG] ä¿å­˜ä»»åŠ¡ {task_id} æ£€æŸ¥ç‚¹")
            logger.info(f"[DEBUG] åˆ†ç±»å™¨æƒé‡å“ˆå¸Œ: {weight_hash}")
            logger.info(f"[DEBUG] åˆ†ç±»å™¨æƒé‡å‡å€¼: {weight_mean:.6f}")
        
        checkpoint = {
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
            'config': self.config,
            'training_history': self.training_history,
            'current_task_id': self.current_task_id,
            'task_performance_history': self.task_performance_history
        }
        
        torch.save(checkpoint, checkpoint_path)
        print(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜åˆ°: {checkpoint_path}")
    
    def load_checkpoint(self, checkpoint_path: str):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        checkpoint = torch.load(checkpoint_path, map_location=self.model.device)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        if checkpoint['scheduler_state_dict'] and self.scheduler:
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        self.config = checkpoint['config']
        self.training_history = checkpoint['training_history']
        self.current_task_id = checkpoint['current_task_id']
        self.task_performance_history = checkpoint['task_performance_history']
        
        print(f"æ£€æŸ¥ç‚¹å·²ä» {checkpoint_path} åŠ è½½")
    
    def save_training_history(self):
        """è·å–æœ€ç»ˆæ‘˜è¦å¹¶ä¿å­˜å®Œæ•´çš„è®­ç»ƒå†å²"""
        history_path = f"{self.output_dir}/training_history.json"
        
        # 1. Get the final summary with all calculated metrics
        final_summary = self.get_training_summary()
        
        # 2. Merge the summary into the main training history object
        # This ensures that calculated fields like 'average_forgetting' are saved.
        self.training_history.update(final_summary)
        
        # 3. Convert numpy arrays to lists for JSON serialization
        serializable_history = {}
        for key, value in self.training_history.items():
            if isinstance(value, dict):
                serializable_history[key] = {k: (v.tolist() if isinstance(v, np.ndarray) else v)
                                           for k, v in value.items()}
            else:
                serializable_history[key] = value.tolist() if isinstance(value, np.ndarray) else value
        
        # 4. Save the merged and serialized history to a file
        with open(history_path, 'w', encoding='utf-8') as f:
            json.dump(serializable_history, f, ensure_ascii=False, indent=4)
        
        print(f"è®­ç»ƒå†å²å·²ä¿å­˜åˆ°: {history_path}")
    
    def get_training_summary(self) -> Dict[str, Any]:
        """è·å–è®­ç»ƒæ‘˜è¦"""
        summary = {
            'total_tasks': len(self.task_performance_history),
            'final_accuracies': {},
            'average_forgetting': 0.0,
            'average_backward_transfer': 0.0, # BWT is complex, keeping it simple for now
            'forgetting_rates': {}
        }
        
        # Diagnostic log: print complete performance history
        logger.info(f"[DIAG] ===== Performance History Details =====")
        for task_id, performances in self.task_performance_history.items():
            logger.info(f"[DIAG] Task {task_id}: {performances}")
        
        # æœ€ç»ˆå‡†ç¡®ç‡
        for task_id, performances in self.task_performance_history.items():
            if performances:
                summary['final_accuracies'][task_id] = performances[-1]

        # è®¡ç®—é—å¿˜ç‡
        all_forgetting_rates = []
        for task_id, performances in self.task_performance_history.items():
            # Can only calculate forgetting if a task has been evaluated more than once
            if len(performances) >= 2:
                # Max accuracy before the final one
                peak_performance = max(performances[:-1])
                final_performance = performances[-1]
                
                forgetting = max(0.0, peak_performance - final_performance)
                summary['forgetting_rates'][task_id] = forgetting
                all_forgetting_rates.append(forgetting)
                
                # Diagnostic log: detailed forgetting rate calculation
                logger.info(f"[DIAG] Task {task_id} forgetting calculation: peak={peak_performance:.4f}, final={final_performance:.4f}, forgetting={forgetting:.4f}")
        
        if all_forgetting_rates:
            summary['average_forgetting'] = np.mean(all_forgetting_rates)
        
        logger.info(f"[DIAG] Average forgetting rate: {summary['average_forgetting']:.4f}")
        
        if self.use_wandb:
            wandb.log({
                'final/average_forgetting': summary['average_forgetting'],
                'final/total_tasks': summary['total_tasks']
            })
            for task_id, acc in summary['final_accuracies'].items():
                wandb.log({f'final/task_{task_id}_accuracy': acc})
        
        return summary